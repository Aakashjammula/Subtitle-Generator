{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Subtitle Generator </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements-gpu.txt\n",
    "#%pip install -r requirements-cpu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt update && sudo apt install ffmpeg -y  # Ubuntu/Debian\n",
    "# brew install ffmpeg  # macOS (Homebrew)\n",
    "# if windows download ffmpeg from https://ffmpeg.org/download.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Audio from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Convert Windows path to WSL-compatible path\n",
    "video_path = \"/mnt/d/DBHH/s1/1.mp4\"\n",
    "audio_path = \"audio.wav\"\n",
    "\n",
    "# Extract audio using FFmpeg\n",
    "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
    "\n",
    "print(\"Audio extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe & Translate Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load Whisper model\n",
    "model_name = \"openai/whisper-large-v2\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load audio file\n",
    "audio_path = \"audio.wav\"\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Segment the audio into 30-second chunks for better transcription\n",
    "chunk_duration = 30  # in seconds\n",
    "num_chunks = int(np.ceil(len(audio) / (chunk_duration * sr)))\n",
    "transcription_segments = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_duration  # Start time in seconds\n",
    "    end = min((i + 1) * chunk_duration, len(audio) / sr)  # End time\n",
    "\n",
    "    # Extract audio chunk\n",
    "    start_sample = int(start * sr)\n",
    "    end_sample = int(end * sr)\n",
    "    audio_chunk = audio[start_sample:end_sample]\n",
    "\n",
    "    # Convert chunk to model input\n",
    "    inputs = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Force translation to English\n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"translate\")\n",
    "\n",
    "    # Transcribe with timestamps and translation\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(inputs, forced_decoder_ids=forced_decoder_ids, max_length=448)\n",
    "\n",
    "    transcript = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Store transcription segment with timestamps\n",
    "    transcription_segments.append({\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"text\": transcript\n",
    "    })\n",
    "\n",
    "# Print detected segments\n",
    "print(\"Detected Speech Segments:\\n\")\n",
    "for seg in transcription_segments:\n",
    "    print(f\"[{timedelta(seconds=int(seg['start']))} --> {timedelta(seconds=int(seg['end']))}]: {seg['text']}\")\n",
    "\n",
    "# Save as an SRT file\n",
    "srt_path = \"subtitles.srt\"\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    return str(timedelta(seconds=int(seconds))).replace(\".\", \",\") + \",000\"\n",
    "\n",
    "with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for i, segment in enumerate(transcription_segments):\n",
    "        start_time = format_timestamp(segment[\"start\"])\n",
    "        end_time = format_timestamp(segment[\"end\"])\n",
    "        srt_file.write(f\"{i+1}\\n{start_time} --> {end_time}\\n{segment['text']}\\n\\n\")\n",
    "\n",
    "print(f\"\\nSubtitles saved to {srt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burn Subtitles into Video (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% ffmpeg -y -i /mnt/d/DBHH/s1/1.mp4 -vf subtitles=subtitles.srt -c:a copy output.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subtitle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
